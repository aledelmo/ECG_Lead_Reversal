{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "The ECG is a time series that measures the electrical activity of the heart. This is the main tool to diagnose heart diseases. Recording an ECG is simple: 3 electrodes are placed at the ends of limbs, and 6 on the anterior chest.This generates 12 time series, called leads, each corresponding to a difference in potential between a pair of electrodes.\n",
    "The electrodes’ position is very important to correctly interpret the ECG. Making the mistake of inverting electrodes compromises interpretation, either because the leads do not explore the expected area (errors in the measures of hypertrophia indices, in the analysis of the ST segment), or because they generate false abnormalities (fake Q waves, error in the heart’s axis…).\n",
    "Inversion errors are frequent (5% of ECGs), and only experts (cardiologists) manage to detect them. But most ECGs are not interpreted by experts: only 30% are, the rest being interpreted by nurses or general practitioners. An algorithm for automatic detection of electrode inversion is therefore paramount to the correct interpretation of ECGs and would improve the quality of diagnosis.\n",
    "This project is intended to make you detect electrode inversion in an ECG. The dataset at your disposal contains ECGs from a cardiology center. An ECG will be labeled as correctly realised (0) or as inverted (1). The goal is to perform binary classification on these ECGs.\n",
    "\n",
    "The key objective of this homework is to propose a model relevant to the task that shows good accuracy in detection of lead inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just defining some standard imports and configs I like to use when working with notebooks\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3, 'Not running on Python 3'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data loading from pickled file.\n",
    "Data:\n",
    "- The training data contains 1400 ECGs and their labels. For each ECG, the data consists of 10 seconds of recording for 12 leads, each sampled at 250Hz.\n",
    "- The testing data contains 2630 ECGs.\n",
    "Each input file therefore contains the ECG signal in the form (n_ecgs, n_samples=2500, n_leads=12).\n",
    "\n",
    "We got for the training set 1400 examples of a 10 seconds ECG (2500 steps sampled at 250Hz) for the 12 leads. The 3-dimensional vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import ast\n",
    "import wfdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/ptb-xl/'\n",
    "sampling_rate = 100\n",
    "\n",
    "class_dict = {'NORM': 0,\n",
    "              'LAFB': 1, 'LPFB': 1, 'IRBBB': 1, 'CLBBB': 1, 'CRBBB': 1, 'IVCD': 1, '_AVB': 1, 'WPW': 1, 'ILBBB': 1,\n",
    "              'STTC': 2, 'NST_': 2, 'ISCA': 2, 'ISC_': 2, 'ISCI': 2,\n",
    "              'AMI': 3, 'IMI': 3, 'LMI': 3,\n",
    "              'LVH': 4, 'LAO': 4, 'LAE': 4, 'RAO': 4, 'RAE': 4,\n",
    "              }\n",
    "\n",
    "\n",
    "def load_raw_data(df, _sampling_rate, _path, drop):\n",
    "    if _sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(_path+f) for i, f in enumerate(df.filename_lr) if i not in drop]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(_path+f) for i, f in enumerate(df.filename_hr) if i not in drop]\n",
    "    data = [signal for signal, meta in data]\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "ds_x = pd.read_csv(os.path.join(path, 'ptbxl_database.csv'), index_col='ecg_id')\n",
    "y_train_raw = [max(ast.literal_eval(code), key=ast.literal_eval(code).get)\n",
    "           if max(ast.literal_eval(code).values()) > 20 else 'NONE'\n",
    "           for code in ds_x.scp_codes]\n",
    "\n",
    "# Cleaning-up unknown labels\n",
    "to_drop = [i for i, c in enumerate(y_train_raw) if c not in class_dict]\n",
    "\n",
    "x_train_raw = load_raw_data(ds_x, sampling_rate, path, drop=to_drop)[:1400]\n",
    "y_train_raw = np.array([class_dict[c] for i, c in enumerate(y_train_raw) if i not in to_drop])[:1400]\n",
    "y_train_raw[y_train_raw > 0] = 1\n",
    "\n",
    "print('Training dataset shape: {} - Training labels shape: {}'.format(\n",
    "    x_train_raw.shape, y_train_raw.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "sns.despine()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(x_train_raw.shape[-1], 1, sharex=True, sharey=True, figsize=[20, 10])\n",
    "for i in range(x_train_raw.shape[-1]):\n",
    "    sns.lineplot(data=x_train_raw[0, :, i], ax=ax[i])\n",
    "plt.show();\n",
    "\n",
    "for i in range(x_train_raw.shape[-1]):\n",
    "    sns.lineplot(data=x_train_raw[0, :, i])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train_raw)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I identified 4 pre-processing steps looking at some artcles: denoising, drift removal, resampling, normalization. \n",
    "1. For denoising and drift removal everyone seems to use a wavelet based analysis to remove high frequency components and a median filter to remove drift below 2Hz.\n",
    "2. For drift removal\n",
    "3. I am not sure resampling is relevant in this case since all data should be already at the same frequency of 250Hz (might obtain improvements with a lower sampling rate? it might reduce noise but lose some information, to be eventually explored). \n",
    "4. For normalization I should bring values in the range of [-1, 1], [0, 1] is also an option. I might adopt a simple normalization or a z score normalization (is it better to normalize lead by lead or use the global statics of the entire sample? to be explored if I got time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pywavelets\n",
    "import pywt\n",
    "#!pip install biosppy\n",
    "from biosppy.signals.tools import filter_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def wavelet_denoising(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\", level=level)\n",
    "    #coeff[0] = np.array([0 if i != 6 else coeff[0][i] for i in range(len(coeff[0]))])\n",
    "\n",
    "    #keep = [0, level - 6 + 1, level - 5 + 1, level - 4 + 1]\n",
    "    #for i in range(len(coeff)):\n",
    "    #    if i not in keep:\n",
    "    #        coeff[i] = np.zeros(len(coeff[i]))\n",
    "\n",
    "    sigma = np.mean(np.absolute(coeff[-1] - np.mean(coeff[-1]))) / 0.6745\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "# db4 or db5 seems to be the wavelets better suited to ecg analysis.\n",
    "# Starting with db4, might explore if time remaining using db5\n",
    "\n",
    "wav = pywt.Wavelet('db4')\n",
    "sns.lineplot(data=x_train_raw[0, :, 11], label='Raw')\n",
    "\n",
    "x_train_denoised = np.zeros(x_train_raw.shape)\n",
    "level = int(np.log2((0.6745 * 100) / 0.5))\n",
    "for s in range(x_train_raw.shape[0]):\n",
    "    for der in range(x_train_raw.shape[-1]):\n",
    "        x_train_denoised[s, :, der] =  wavelet_denoising(x_train_raw[s, :, der], wavelet=wav,\n",
    "                                                         level=level)\n",
    "\n",
    "sns.lineplot(data=x_train_denoised[0, :, 11], label='Wavelet Denoising db4 - {}'.format(level))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not find satisfying results removeing the low frequency components of the approximate coefficients (might be doing it wrong?) Thresholding based on the mean absolute deviation of the first detailed coefficient seems promising.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=x_train_denoised[0, :, 11], label='Wavelet Denoising db4 - {}'.format(level))\n",
    "\n",
    "order = int(0.3 * sampling_rate)\n",
    "\n",
    "x_train_nodrift = np.zeros(x_train_denoised.shape)\n",
    "for s in range(x_train_denoised.shape[0]):\n",
    "    for der in range(x_train_denoised.shape[-1]):\n",
    "        x_train_nodrift[s, :, der], _, _ = filter_signal(signal=x_train_denoised[s, :, der], ftype=\"FIR\", band=\"bandpass\",\n",
    "                                                 order=order, frequency=[2.5, 40],\n",
    "                                                 sampling_rate=sampling_rate)\n",
    "\n",
    "sns.lineplot(data=x_train_nodrift[0, :, 11], label='Baseline Drift Removed - Bandpass order {}'. format(order))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_norm(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    if std > 0:\n",
    "        x = (x - mean) / std\n",
    "    else:\n",
    "        x *= 0.\n",
    "    return x\n",
    "\n",
    "x_train_centered = np.zeros(x_train_denoised.shape)\n",
    "for s in range(x_train_nodrift.shape[0]):\n",
    "    for der in range(x_train_nodrift.shape[-1]):\n",
    "        x_train_centered[s, :, der] = z_score_norm(x_train_nodrift[s, :, der])\n",
    "        \n",
    "sns.lineplot(data=x_train_centered[0, :, 11], label='Baseline Drift Removed')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "skf.get_n_splits(x_train_centered, y_train_raw)\n",
    "\n",
    "train_folds = []\n",
    "val_folds = []\n",
    "for train_index, test_index in skf.split(x_train_centered, y_train_raw):\n",
    "    train_folds.append((x_train_centered[train_index], y_train_raw[train_index]))\n",
    "    val_folds.append((x_train_centered[test_index],  y_train_raw[test_index]))\n",
    "\n",
    "# For the moment let's just work on one fold \n",
    "train_ds_fold0 = train_folds[0]\n",
    "val_ds_fold0 = val_folds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the deep learning framework I decided to use Tensorflow 2.x (2.8 in my case). Can work on gpu using memory growth strategy and possibility of enabling eager debugging for @tf.functions. Tensorboard logs are stored in the ./tf_logs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "debug_mode = False # Reminder to myself: set to false before sending \n",
    "\n",
    "# Tensorflow 2.x standard import + tensorboard\n",
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "%load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# GPU accelerated processing\n",
    "if use_gpu:\n",
    "    gpu_list = tf.config.list_physical_devices('GPU')\n",
    "    print(\"GPUs Available:\")\n",
    "    if gpu_list:\n",
    "        try:\n",
    "            for gpu in gpu_list:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpu_list), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "# Debugging\n",
    "tf.config.run_functions_eagerly(debug_mode)\n",
    "if debug_mode:\n",
    "    tf.data.experimental.enable_debug_mode()\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "\n",
    "# Empty logs and cache\n",
    "!rm -rf ./tf_logs/ \n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "initial_lr = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(train_ds_fold0)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(val_ds_fold0)\n",
    "\n",
    "train_ds = train_ds.batch(batch_size, num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                deterministic=False).prefetch(tf.data.experimental.AUTOTUNE\n",
    "                                                             ).cache().shuffle(len(train_ds),\n",
    "                                                                               reshuffle_each_iteration=True)\n",
    "val_ds = val_ds.batch(batch_size, num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                            deterministic=True).prefetch(tf.data.experimental.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters=64, kernel_size=16, use_final_conv=True, use_final_pooling=True,\n",
    "                 name='ConvBlock', **kwargs):\n",
    "        super(ConvBlock, self).__init__(name=name, **kwargs)\n",
    "        self.use_final_conv = use_final_conv\n",
    "        self.use_final_pooling = use_final_pooling\n",
    "\n",
    "        self.batch_1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        self.act_1 = tf.keras.layers.ReLU()\n",
    "        self.drop_1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.conv_1 = tf.keras.layers.Conv1D(filters, kernel_size=kernel_size, padding='same', \n",
    "                                             kernel_initializer='he_normal', activation=None)\n",
    "        self.batch_2 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        self.act_2 = tf.keras.layers.ReLU()\n",
    "        self.drop_2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.conv_2 = tf.keras.layers.Conv1D(filters, kernel_size=kernel_size, padding='same',\n",
    "                                             kernel_initializer='he_normal', activation=None)\n",
    "        \n",
    "        self.pool_fin = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        \n",
    "        self.conv_par = tf.keras.layers.Conv1D(filters, kernel_size=1)\n",
    "        self.pool_par = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        if self.use_final_conv:\n",
    "            xshort = self.conv_par(inputs, training=training)\n",
    "        else:\n",
    "            xshort = inputs\n",
    "        \n",
    "        x1 = self.batch_1(inputs, training=training)\n",
    "        x1 = self.act_1(x1, training=training)\n",
    "        x1 = self.drop_1(x1, training=training)\n",
    "        x1 = self.conv_1(x1, training=training)\n",
    "        x1 = self.batch_2(x1, training=training)\n",
    "        x1 = self.act_2(x1, training=training)\n",
    "        x1 = self.drop_2(x1, training=training)\n",
    "        x1 = self.conv_2(x1, training=training)\n",
    "        \n",
    "        if self.use_final_pooling:\n",
    "            x1 = self.pool_fin(x1, training=training)\n",
    "            x2 = self.pool_par(xshort, training=training)\n",
    "        else:\n",
    "            x2 = xshort\n",
    "        \n",
    "        return x1 + x2\n",
    "\n",
    "        \n",
    "class ResNet1D(tf.keras.models.Model):\n",
    "    def __init__(self, base_filters=64, kernel_size = 16, n_class=2, inshape=(1000,12)):\n",
    "        super(ResNet1D, self).__init__(self)\n",
    "\n",
    "        '-----------------------'\n",
    "        self.conv_pre = tf.keras.layers.Conv1D(base_filters, kernel_size=kernel_size, padding='same', \n",
    "                                               kernel_initializer='he_normal', activation=None,\n",
    "                                               input_shape=inshape)\n",
    "        self.batch_pre = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        self.act_pre = tf.keras.layers.ReLU()\n",
    "        '-----------------------'\n",
    "        self.conv_l1 = tf.keras.layers.Conv1D(base_filters, kernel_size=kernel_size, padding='same',\n",
    "                                              kernel_initializer='he_normal', activation=None)\n",
    "        self.batch_l1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        self.act_l1 = tf.keras.layers.ReLU()\n",
    "        self.drop_l1 = tf.keras.layers.Dropout(0.5)\n",
    "        self.conv_l2 = tf.keras.layers.Conv1D(base_filters, kernel_size=kernel_size, padding='same', \n",
    "                                              kernel_initializer='he_normal', activation=None)\n",
    "        self.pool_l1 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        \n",
    "        self.pool_r1 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        '-----------------------'\n",
    "        self.conv_block1 = ConvBlock(filters=base_filters * 1, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=False)\n",
    "        self.conv_block2 = ConvBlock(filters=base_filters * 1, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block3 = ConvBlock(filters=base_filters * 1, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=False)\n",
    "        self.conv_block4 = ConvBlock(filters=base_filters * 1, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block5 = ConvBlock(filters=base_filters * 2, kernel_size=kernel_size,\n",
    "                                     use_final_conv=True, use_final_pooling=False)\n",
    "        self.conv_block6 = ConvBlock(filters=base_filters * 2, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block7 = ConvBlock(filters=base_filters * 2, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=False)\n",
    "        self.conv_block8 = ConvBlock(filters=base_filters * 2, kernel_size=kernel_size,\n",
    "                                     use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block9 = ConvBlock(filters=base_filters * 3, kernel_size=kernel_size,\n",
    "                                     use_final_conv=True, use_final_pooling=False)\n",
    "        self.conv_block10 = ConvBlock(filters=base_filters * 3, kernel_size=kernel_size,\n",
    "                                      use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block11 = ConvBlock(filters=base_filters * 3, kernel_size=kernel_size,\n",
    "                                      use_final_conv=False, use_final_pooling=False)\n",
    "        self.conv_block12 = ConvBlock(filters=base_filters * 3, kernel_size=kernel_size,\n",
    "                                      use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block13 = ConvBlock(filters=base_filters * 4, kernel_size=kernel_size,\n",
    "                                      use_final_conv=True, use_final_pooling=False)\n",
    "        self.conv_block14 = ConvBlock(filters=base_filters * 4, kernel_size=kernel_size,\n",
    "                                      use_final_conv=False, use_final_pooling=True)\n",
    "        self.conv_block15 = ConvBlock(filters=base_filters * 4, kernel_size=kernel_size,\n",
    "                                      use_final_conv=False, use_final_pooling=False)\n",
    "        '-----------------------'\n",
    "        self.batch_final = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        self.act_final = tf.keras.layers.ReLU()\n",
    "        self.f = tf.keras.layers.Flatten()\n",
    "        self.dense_final = tf.keras.layers.Dense(n_class, activation='softmax')\n",
    "        \n",
    "\n",
    "    @tf.function()\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv_pre(inputs, training=training)\n",
    "        x = self.batch_pre(x, training=training)\n",
    "        x_pre = self.act_pre(x, training=training)\n",
    "        \n",
    "        x_branch_l = self.conv_l1(x_pre, training=training)\n",
    "        x_branch_l = self.batch_l1(x_branch_l, training=training)\n",
    "        x_branch_l = self.act_l1(x_branch_l, training=training)\n",
    "        x_branch_l = self.drop_l1(x_branch_l, training=training)\n",
    "        x_branch_l = self.conv_l2(x_branch_l, training=training)\n",
    "        x_branch_l = self.pool_l1(x_branch_l, training=training)\n",
    "        \n",
    "        x_branch_r = self.pool_r1(x_pre, training=training)\n",
    "                \n",
    "        x_in = x_branch_l + x_branch_r\n",
    "        \n",
    "        x_conv = self.conv_block1(x_in, training=training)\n",
    "        x_conv = self.conv_block2(x_conv, training=training)\n",
    "        x_conv = self.conv_block3(x_conv, training=training)\n",
    "        x_conv = self.conv_block4(x_conv, training=training)\n",
    "        x_conv = self.conv_block5(x_conv, training=training)\n",
    "        x_conv = self.conv_block6(x_conv, training=training)\n",
    "        x_conv = self.conv_block7(x_conv, training=training)\n",
    "        x_conv = self.conv_block8(x_conv, training=training)\n",
    "        x_conv = self.conv_block9(x_conv, training=training)\n",
    "        x_conv = self.conv_block10(x_conv, training=training)\n",
    "        x_conv = self.conv_block11(x_conv, training=training)\n",
    "        x_conv = self.conv_block12(x_conv, training=training)\n",
    "        x_conv = self.conv_block13(x_conv, training=training)\n",
    "        x_conv = self.conv_block14(x_conv, training=training)\n",
    "        x_conv = self.conv_block15(x_conv, training=training)\n",
    "        \n",
    "        x_final = self.batch_final(x_conv, training=training)\n",
    "        x_final = self.act_final(x_final, training=training)\n",
    "        x_final = self.f(x_final)\n",
    "        x_final = self.dense_final(x_final, training=training)\n",
    "        \n",
    "        return x_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(inshape, base_filters, dense_filters, n_convs = 3, n_dense = 2, ):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    m = 1\n",
    "    for i in range(n_convs):\n",
    "        if i == 0:\n",
    "            model.add(tf.keras.layers.Conv1D(base_filters * m, 3, padding='same',\n",
    "                                             activation='relu', input_shape=inshape))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.Conv1D(base_filters * m, 3, padding='same',\n",
    "                                             activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "        model.add(tf.keras.layers.MaxPooling1D())\n",
    "        m += m\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    if n_dense > 1:\n",
    "        m = 2 * (n_dense - 1)\n",
    "    else:\n",
    "        m = 1\n",
    "    for i in range(n_dense):\n",
    "        model.add(tf.keras.layers.Dense(dense_filters * m, activation='relu'))\n",
    "        m = m // 2\n",
    "    model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "#model = get_model((1000, 12), 128, 8, 1, 1)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metric_fn = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tf_logs/hparam_tuning --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([20]))\n",
    "HP_BATCH = hp.HParam('batch', hp.Discrete([4]))\n",
    "HP_LR = hp.HParam('initial_lr', hp.Discrete([1e-6]))\n",
    "HP_FILTERS = hp.HParam('base_filters', hp.Discrete([64]))\n",
    "HP_DENSEFILTERS = hp.HParam('dense_filters', hp.Discrete([16]))\n",
    "HP_N_CONV = hp.HParam('n_conv_filters', hp.Discrete([2]))\n",
    "HP_N_DENSE = hp.HParam('n_dense_filters', hp.Discrete([3]))\n",
    "\n",
    "session_num = 0\n",
    "run_name = 'run'\n",
    "\n",
    "def hyper_parameters_config():\n",
    "    for e in HP_EPOCHS.domain.values:\n",
    "        for lr in HP_BATCH.domain.values:\n",
    "            for a in HP_LR.domain.values:\n",
    "                for bf in HP_FILTERS.domain.values:\n",
    "                    for d in HP_DENSEFILTERS.domain.values:\n",
    "                        for nc in HP_N_CONV.domain.values:\n",
    "                            for nd in HP_N_DENSE.domain.values:\n",
    "                                yield {\n",
    "                                    HP_EPOCHS: e,\n",
    "                                    HP_BATCH: lr,\n",
    "                                    HP_LR: a,\n",
    "                                    HP_FILTERS: bf,\n",
    "                                    HP_DENSEFILTERS: d,\n",
    "                                    HP_N_CONV: nc,\n",
    "                                    HP_N_DENSE: nd,\n",
    "                                }\n",
    "\n",
    "for hparams in hyper_parameters_config():\n",
    "    run_name = 'run-{}'.format(session_num)\n",
    "    print('\\n\\n--- Starting trial: {}'.format(run_name))\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "    run_logdir = 'tf_logs/hparam_tuning/' + run_name\n",
    "    summary_writer = tf.summary.create_file_writer(run_logdir)\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        hp.hparams(hparams)\n",
    "        #model = get_model((1000, 12), hparams[HP_FILTERS], hparams[HP_DENSEFILTERS], hparams[HP_N_CONV], hparams[HP_N_DENSE])\n",
    "        model = ResNet1D(hparams[HP_FILTERS], hparams[HP_DENSEFILTERS])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy',],)\n",
    "\n",
    "        model.fit(x=train_ds, validation_data=val_ds,\n",
    "                  batch_size=batch_size, epochs=hparams[HP_EPOCHS], verbose=2, \n",
    "                  callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "                             tf.keras.callbacks.TensorBoard(log_dir=run_logdir, histogram_freq=1)],\n",
    "                  use_multiprocessing=True)\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "\n",
    "    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(val_ds_fold0[0]), axis=1)\n",
    "y_true = val_ds_fold0[1]\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(confusion_mtx,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
